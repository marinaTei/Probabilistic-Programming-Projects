{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"GaxVWnUvB2z4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ce6aac70-7ce4-41ca-ec6d-5cdf4eb9d57a","executionInfo":{"status":"ok","timestamp":1582891025997,"user_tz":-120,"elapsed":664,"user":{"displayName":"Marina Teianu","photoUrl":"","userId":"18068248439346164178"}}},"source":["import numpy as np\n","import pandas as pd\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","# !pip install sklearn\n","# !pip install tensorflow\n","# !pip install edward2\n","from tensorflow_probability import edward2 as ed\n","from keras.utils import to_categorical\n","import tensorflow.keras as keras\n","import seaborn as sns\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"dygWQBVp87gt","colab_type":"text"},"source":["**Data for Sanity Check**\n"]},{"cell_type":"code","metadata":{"id":"_DU_GLtlCAtw","colab_type":"code","colab":{}},"source":["x = np.random.randn(140, 2)\n","Y = np.tanh(x[:, 0]+ x[:, 1])\n","Y = 1. / (1. + np.exp(-(Y+Y)))\n","Y = Y > 0.5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qjWq1qTT9EDh","colab_type":"code","colab":{}},"source":["X_1 = x[Y]\n","X_0 = x[np.logical_not(Y)]\n","\n","sns.scatterplot(X_1[:, 0], X_1[:, 1])\n","sns.scatterplot(X_0[:, 0], X_0[:, 1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E8U5Jp0--aLE","colab_type":"text"},"source":["**Two Circles Dataset**"]},{"cell_type":"code","metadata":{"id":"Y_YgX5mm6sg3","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_circles\n","\n","t = make_circles(n_samples=1000, noise=0.04)\n","labels = t[1]\n","data = t[0]\n","\n","cls_1 = data[labels == 1]\n","cls_0 = data[np.logical_not(labels)]\n","\n","sns.scatterplot(cls_1[:, 0], cls_1[:, 1])\n","sns.scatterplot(cls_0[:, 0], cls_0[:, 1])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uMYiMuEGBZcK","colab_type":"code","colab":{}},"source":["# for two circles dataset\n","# y_binary = to_categorical(labels)\n","# X = data\n","\n","# train_samples = 900\n","# test_samples = 100\n","# hidden_layers = 16\n","\n","# for sanity check\n","y_binary = to_categorical(Y)\n","X = x\n","\n","train_samples = 120\n","test_samples = 20\n","hidden_layers = 4"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ERK6Km2BCAUS","colab_type":"text"},"source":["**Neural Network** <br>\n","with <br>\n","an input layer that has 2 neurons (activation function is *tanh*), <br>\n","one hidden layer with *hidden_layer* neurons (activation function is *tanh*) <br>\n","and an output layer with 1 neuron (activation function is *sigmoid*)<br>\n","<br>\n","optimizer: *SGD* and the loss is *binary crossentropy*"]},{"cell_type":"code","metadata":{"id":"6GPHKUPtCESX","colab_type":"code","colab":{}},"source":["# define model\n","model = keras.models.Sequential()\n","model.add(keras.layers.Dense(units=2, input_dim=X.shape[1], activation='tanh'))\n","model.add(keras.layers.Dense(units=hidden_layers, input_dim=2, activation='tanh'))\n","model.add(keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","# define optimizer and compile model\n","sgd_optimizer = keras.optimizers.SGD(lr=0.001, decay=1e-7, momentum=0.9)\n","model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy')\n","\n","# fitting the net\n","history = model.fit(X[:train_samples, :], y_binary[:train_samples, 0], batch_size=64, epochs=1000, verbose=2, validation_split=0.1)\n","\n","# saving the weights for compering them with the bayesian_nn ones \n","nn_summary = model.summary()\n","nn_weights = model.get_weights()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GYvLdjNqCODp","colab_type":"code","colab":{}},"source":["# evaluate model\n","score = model.evaluate(X[train_samples:, :], y_binary[train_samples:, 0])\n","print('acc: ', 1-score)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2_momPfmdP2U","colab_type":"text"},"source":["**Setting prior on weight for the bayesian neural network**"]},{"cell_type":"code","metadata":{"id":"ZlFwRTUGCTfh","colab_type":"code","colab":{}},"source":["\n","def prior_on_weights(X):\n","  # layers weights\n","  weight_lay1 = tf.convert_to_tensor(ed.Normal(name='w1', loc=np.zeros([X.shape[1], hidden_layers], dtype='float32'), \n","                                               scale=np.ones([X.shape[1], hidden_layers],  dtype='float32')), dtype=tf.float32)\n","  weight_lay2 = tf.convert_to_tensor(ed.Normal(name='w2', loc=np.zeros([hidden_layers, 1],  dtype='float32'), \n","                                               scale=np.ones([hidden_layers, 1],  dtype='float32')), dtype=tf.float32)\n","  # layers biases \n","  bias_lay1 = tf.convert_to_tensor(ed.Normal(name='b1', loc=np.zeros([1, hidden_layers],  dtype='float32'), \n","                                             scale=np.ones([1, hidden_layers],  dtype='float32')), dtype=tf.float32)\n","  bias_lay2 = tf.convert_to_tensor(ed.Normal(name='b2', loc=np.zeros(1,  dtype='float32'), \n","                                             scale=np.ones(1,  dtype='float32')), dtype=tf.float32)\n","  \n","  # layers logic\n","  out_lay1 = tf.nn.tanh(tf.add(np.tensordot(X, weight_lay1, axes=1), bias_lay1))\n","  out_lay2 = keras.activations.sigmoid(np.tensordot(out_lay1, weight_lay2, axes=1)+ bias_lay2)\n","\n","  # output layer as a Bernoulli disribution of second layer output\n","  output = ed.Bernoulli(name='out', logits=out_lay2[:, 0], dtype=tf.float32)\n","  return output\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e33_Xq-lCphU","colab_type":"code","colab":{}},"source":["# initial values for weights\n","q_w1 = tf.random.normal([], mean=np.zeros([2, hidden_layers]), stddev=np.ones([2, hidden_layers]), dtype=tf.float32)\n","q_w2 = tf.random.normal([], mean=np.zeros([hidden_layers, 1]), stddev=np.ones([hidden_layers, 1]), dtype=tf.float32)\n","\n","# initial values for biases\n","q_b1 = tf.random.normal([], mean=np.zeros([1, hidden_layers]), stddev=np.ones([1, hidden_layers]), dtype=tf.float32)\n","q_b2 = tf.random.normal([], mean=np.zeros(1), stddev=np.ones(1), dtype=tf.float32)\n","\n","# convert train data to tensors\n","x_train = tf.convert_to_tensor(X[:train_samples, :], dtype=tf.float32)\n","y_train = tf.convert_to_tensor(y_binary[:train_samples, 0], dtype=tf.float32)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdDzIPmyDThA","colab_type":"code","colab":{}},"source":["# define target probability distribution\n","log_joint = ed.make_log_joint_fn(prior_on_weights)\n","\n","def target_log_prob_fn(w1, b1, w2, b2):\n","        return log_joint(x_train, w1=w1, b1=b1, w2=w2, b2=b2, out=y_train)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tn2Anj4Cdz_r","colab_type":"code","colab":{}},"source":["#  define method of sampling\n"," hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(target_log_prob_fn=target_log_prob_fn, step_size=0.01, num_leapfrog_steps=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZKIc1aLd6Dz","colab_type":"code","colab":{}},"source":["# sample mcmc starting from initial values \n","states = tfp.mcmc.sample_chain(num_results=  1000, current_state=[q_w1, q_b1, q_w2, q_b2], kernel=hmc_kernel, num_burnin_steps=500)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"06IzsGyPJHFt","colab_type":"text"},"source":["**Compute weights and biases as a mean over samples**"]},{"cell_type":"code","metadata":{"id":"y7GByzJ2Lpo8","colab_type":"code","colab":{}},"source":["w1 = np.mean(states[0][kernel_results.is_accepted.numpy()], 0)\n","b1 = np.mean(states[1][kernel_results.is_accepted.numpy()], 0)\n","w2 = np.mean(states[2][kernel_results.is_accepted.numpy()], 0)\n","b2 = np.mean(states[3][kernel_results.is_accepted.numpy()], 0)\n","\n","# print(w1)\n","# print(b1)\n","# print(w2)\n","# print(b2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S5n60HHUI0zr","colab_type":"text"},"source":["**Predict labels for test data**"]},{"cell_type":"code","metadata":{"id":"mQOOHZIHVjV3","colab_type":"code","colab":{}},"source":["x_test = tf.convert_to_tensor(X[train_samples:, :], dtype=tf.float32)\n","y1 = tf.math.tanh(tf.add(tf.matmul(x_test, w1), b1))\n","y2 = tf.math.sigmoid(tf.add(tf.matmul(y1, w2), b2))\n","\n","y_predicted = y2.numpy()\n","y_predicted_classes = np.array([0 if y < .5 else 1 for y in y_predicted])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDhbNRxPIhe0","colab_type":"text"},"source":["**Bayesian Neural Network Accuracy**\n","\n"]},{"cell_type":"code","metadata":{"id":"amofwKucWcyJ","colab_type":"code","colab":{}},"source":["# !pip install sklearn\n","import sklearn.metrics as met\n","\n","# define true labels for sanity check\n","true_labels = np.array([0 if y == False else True for y in y_binary[train_samples:, 0]])\n","\n","# define true labels for two circles\n","# true_labels = labels[train_samples:]\n","\n","print('bayesian_nn acc: ', met.accuracy_score(true_labels, y_predicted_classes))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4lOFOKrJYGq","colab_type":"text"},"source":["**Compare the two approches**"]},{"cell_type":"code","metadata":{"id":"4dESfc_kJdgl","colab_type":"code","colab":{}},"source":["print('nn_weights: ', nn_weights)\n","print('bnn_weights: ')\n","print('w1: ', w1)\n","print('b1: ', b1)\n","print('w2: ', w2)\n","print('b2: ', b2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rYaf3womrdP-","colab_type":"text"},"source":["**Multiclass Classification Neural Network**"]},{"cell_type":"code","metadata":{"id":"UgCDvaJJZUPj","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_circles\n","\n","t = make_circles(n_samples=1000, noise=0.04)\n","t2 = make_circles(n_samples=1000, noise=0.1, random_state=0, factor=0.5)\n","\n","labels = t[1]\n","data = t[0]\n","\n","labels2 = t2[1]\n","data2 = t2[0]\n","\n","cls_1 = data[labels == 1]\n","cls_0 = data[np.logical_not(labels)]\n","\n","cls_12 = data2[labels2 == 1]\n","cls_02 = data2[np.logical_not(labels2)]\n","\n","sns.scatterplot(cls_1[:, 0], cls_1[:, 1])\n","sns.scatterplot(cls_0[:, 0], cls_0[:, 1])\n","sns.scatterplot(cls_12[:, 0], cls_12[:, 1])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5gvEFYFtj2_","colab_type":"code","colab":{}},"source":["dataset = pd.DataFrame(data)\n","dataset['labels'] = labels\n","aux = pd.DataFrame(data2[labels2 == 1])\n","aux['labels'] = 2\n","dataset = dataset.append(aux)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgVrk6F1tkB5","colab_type":"code","colab":{}},"source":["\n","n_samples = dataset.shape[0]\n","n_features = dataset.shape[1]\n","n_classes = len(set(dataset.labels))\n","n_hidden = 16"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7obe9vBEvN9b","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing as pr\n","data_train, data_test, labels_train, labels_test = train_test_split(pr.scale(np.array(dataset.iloc[:, :2])), np.array(dataset.labels), test_size=0.2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IN7pXE-srqtJ","colab_type":"code","colab":{}},"source":["# define model\n","model2 = keras.models.Sequential()\n","\n","model2.add(keras.layers.Dense(units=n_features, activation='relu'))\n","model2.add(keras.layers.Dense(units=n_hidden, activation='relu'))\n","model2.add(keras.layers.Dense(units=n_classes, activation='softmax'))\n"," \n","# define optimizer and compile model\n","sgd_optimizer = keras.optimizers.SGD(lr=0.001,decay=1e-6, momentum=0.9)\n","model2.compile(optimizer=sgd_optimizer, loss='sparse_categorical_crossentropy')\n","\n","# fitting the net\n","history = model2.fit(data_train, labels_train, epochs=1200, verbose=2, validation_split=0.1, batch_size=64)\n","\n","# saving the weights for compering them with the bayesian_nn ones \n","nn2_summary = model2.summary()\n","nn2_weights = model2.get_weights()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfD5NkRGwZ-H","colab_type":"code","colab":{}},"source":["# print(labels_test.shape)\n","score2 = model2.evaluate(data_test, labels_test)\n","print('acc: ', 1-score2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"71a2S5qur4Ep","colab_type":"code","colab":{}},"source":["\n","def prior_on_weights_multiclass(X):\n","  \n","  # layers weights\n","  weight_lay1 = tf.convert_to_tensor(ed.Normal(name='w1', loc=np.zeros([2, n_hidden], dtype='float32'), \n","                                               scale=np.ones([2, n_hidden],  dtype='float32')), dtype=tf.float32)\n","  weight_lay2 = tf.convert_to_tensor(ed.Normal(name='w2', loc=np.zeros([n_hidden, n_classes],  dtype='float32'), \n","                                               scale=np.ones([n_hidden, n_classes],  dtype='float32')), dtype=tf.float32)\n","  # layers biases \n","  bias_lay1 = tf.convert_to_tensor(ed.Normal(name='b1', loc=np.zeros([1, n_hidden],  dtype='float32'), \n","                                             scale=np.ones([1, n_hidden],  dtype='float32')), dtype=tf.float32)\n","  bias_lay2 = tf.convert_to_tensor(ed.Normal(name='b2', loc=np.zeros(n_classes,  dtype='float32'), \n","                                             scale=np.ones(n_classes,  dtype='float32')), dtype=tf.float32)\n","  \n","  # layers logic\n","  out_lay1 = tf.nn.relu(tf.add(np.tensordot(X, weight_lay1, axes=1), bias_lay1))\n","  out_lay2 = keras.activations.softmax(np.tensordot(out_lay1, weight_lay2, axes=1)+ bias_lay2)\n","\n","  # output layer as a Categorical disribution of second layer output\n","  output = ed.Categorical(name='out', logits=out_lay2, dtype=tf.float32)\n","  return output\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"16HIG6kwyNBq","colab_type":"code","colab":{}},"source":["# initial values for weights\n","q_w1 = tf.random.normal([], mean=np.zeros([2, n_hidden]), stddev=np.ones([2, n_hidden]), dtype=tf.float32)\n","q_w2 = tf.random.normal([], mean=np.zeros([n_hidden, n_classes]), stddev=np.ones([n_hidden, 1]), dtype=tf.float32)\n","\n","# initial values for biases\n","q_b1 = tf.random.normal([], mean=np.zeros([1, n_hidden]), stddev=np.ones([1, n_hidden]), dtype=tf.float32)\n","q_b2 = tf.random.normal([], mean=np.zeros(n_classes), stddev=np.ones(n_classes), dtype=tf.float32)\n","\n","# convert train data to tensors\n","x_train = tf.convert_to_tensor(data_train, dtype=tf.float32)\n","y_train = tf.convert_to_tensor(labels_train, dtype=tf.float32)\n","print(y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DomN9k0ZyNQb","colab_type":"code","colab":{}},"source":["# define target probability distribution\n","log_joint = ed.make_log_joint_fn(prior_on_weights_multiclass)\n","\n","def target_log_prob_fn(w1, b1, w2, b2):\n","        return log_joint(x_train, w1=w1, b1=b1, w2=w2, b2=b2, out=y_train)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-d_wTK7y0bj","colab_type":"code","colab":{}},"source":["#  define method of sampling\n"," hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(target_log_prob_fn=target_log_prob_fn, step_size=0.01, num_leapfrog_steps=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ouxf6CoNy5Th","colab_type":"code","colab":{}},"source":["# sample mcmc starting from initial values \n","states = tfp.mcmc.sample_chain(num_results=100, current_state=[q_w1, q_b1, q_w2, q_b2], kernel=hmc_kernel, num_burnin_steps=50)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6jFCipJ3XzR","colab_type":"code","colab":{}},"source":["w1 = np.mean(states[0][kernel_results.is_accepted.numpy()], 0)\n","b1 = np.mean(states[1][kernel_results.is_accepted.numpy()], 0)\n","w2 = np.mean(states[2][kernel_results.is_accepted.numpy()], 0)\n","b2 = np.mean(states[3][kernel_results.is_accepted.numpy()], 0)\n","\n","print(w1)\n","print(b1)\n","print(w2)\n","print(b2)\n","\n","# print(len(states))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKv23U1I3xj7","colab_type":"code","colab":{}},"source":["\n","l1 = tf.nn.relu(tf.add(tf.matmul(tf.convert_to_tensor(data_test, dtype=tf.float32), w1), b1))\n","l2 = tf.math.softmax(tf.add(tf.matmul(l1, w2), b2))\n","\n","y_predicted = y2.numpy().argmax(axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTca4x7V4lZr","colab_type":"code","colab":{}},"source":["print(y_predicted)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGOF-T-X4TKf","colab_type":"code","colab":{}},"source":["# !pip install sklearn\n","import sklearn.metrics as met\n","\n","print('bayesian_nn_multiclass acc: ', met.accuracy_score(labels_test, y_predicted))\n"],"execution_count":0,"outputs":[]}]}